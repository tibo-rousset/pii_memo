{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d7d2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.12/dist-packages (1.43.3)\n",
      "Requirement already satisfied: importlib-metadata>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from circuitsvis) (8.7.0)\n",
      "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from circuitsvis) (1.26.4)\n",
      "Requirement already satisfied: torch>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from circuitsvis) (2.9.0+cu126)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.1->circuitsvis) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.1->circuitsvis) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.1->circuitsvis) (3.0.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[31m================================================================================\u001b[m\n",
      "\u001b[31m================================================================================\u001b[m\n",
      "\n",
      "  \u001b[1m\u001b[33m                            DEPRECATION WARNING                            \u001b[m\n",
      "\n",
      "    \u001b[1m\u001b[4m Node.js 16.x is no longer actively supported!\u001b[m\n",
      "\n",
      "  \u001b[1mYou will not receive security or critical stability updates\u001b[m for this version.\n",
      "\n",
      "  You should migrate to a supported version of Node.js as soon as possible.\n",
      "  Use the installation script that corresponds to the version of Node.js you\n",
      "  wish to install. e.g.\n",
      "  \n",
      "   * \u001b[31mhttps://deb.nodesource.com/setup_16.x — Node.js 16 \"Gallium\" \u001b[1m(deprecated)\u001b[m\n",
      "   * \u001b[32mhttps://deb.nodesource.com/setup_18.x — Node.js 18 \"Hydrogen\" (Maintenance)\u001b[m\n",
      "   * \u001b[31mhttps://deb.nodesource.com/setup_19.x — Node.js 19 \"Nineteen\" \u001b[1m(deprecated)\u001b[m\n",
      "   * \u001b[1m\u001b[32mhttps://deb.nodesource.com/setup_20.x — Node.js 20 LTS \"Iron\" (recommended)\u001b[m\n",
      "   * \u001b[32mhttps://deb.nodesource.com/setup_21.x — Node.js 21 \"Iron\" (current)\u001b[m\n",
      "   \n",
      "\n",
      "\n",
      "  Please see \u001b[1mhttps://github.com/nodejs/Release\u001b[m for details about which\n",
      "  version may be appropriate for you.\n",
      "\n",
      "  The \u001b[32m\u001b[1mNodeSource\u001b[m Node.js distributions repository contains\n",
      "  information both about supported versions of Node.js and supported Linux\n",
      "  distributions. To learn more about usage, see the repository:\n",
      "   \u001b[4m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n",
      "\n",
      "\u001b[31m================================================================================\u001b[m\n",
      "\u001b[31m================================================================================\u001b[m\n",
      "\n",
      "Continuing in 10 seconds ...\n",
      "\n",
      "\u001b[38;5;79m2025-11-27 18:21:26 - Installing pre-requisites\u001b[0m\n",
      "Hit:1 https://cli.github.com/packages stable InRelease\n",
      "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease     \u001b[0m\n",
      "Hit:3 https://deb.nodesource.com/node_16.x nodistro InRelease                  \u001b[0m\n",
      "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease                         \u001b[0m\n",
      "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease               \u001b[0m\n",
      "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease                     \u001b[0m\u001b[33m\n",
      "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                 \u001b[0m\n",
      "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease               \n",
      "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ca-certificates is already the newest version (20240203~22.04.1).\n",
      "curl is already the newest version (7.81.0-1ubuntu1.21).\n",
      "gnupg is already the newest version (2.2.27-3ubuntu2.4).\n",
      "apt-transport-https is already the newest version (2.4.14).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
      "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease     \u001b[0m\n",
      "Hit:3 https://cli.github.com/packages stable InRelease                         \u001b[0m\n",
      "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:5 https://deb.nodesource.com/node_16.x nodistro InRelease                  \u001b[0m\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease                         \u001b[0m\u001b[33m\n",
      "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease                     \u001b[0m\n",
      "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                 \u001b[0m\u001b[33m\u001b[33m\n",
      "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\u001b[33m\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
      "\u001b[1;32m2025-11-27 18:21:36 - Repository configured successfully. To install Node.js, run: apt install nodejs -y\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "nodejs is already the newest version (16.20.2-1nodesource1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DEVELOPMENT_MODE = False\n",
    "# Detect if we're running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Install if in Colab\n",
    "if IN_COLAB:\n",
    "    %pip install transformer_lens\n",
    "    %pip install circuitsvis\n",
    "    %pip install pandas\n",
    "    # Install a faster Node version\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa\n",
    "\n",
    "# Hot reload in development mode & not running on the CD\n",
    "if not IN_COLAB:\n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "    if not ip.extension_manager.loaded:\n",
    "        ip.extension_manager.load('autoreload')\n",
    "        %autoreload 2\n",
    "        \n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60eab3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using renderer: colab\n"
     ]
    }
   ],
   "source": [
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f59d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa09611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "import transformer_lens.utils as utils\n",
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f256cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed37c1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-d9c5339d-5268\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-d9c5339d-5268\",\n",
       "      Hello,\n",
       "      {\"name\": \"George\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7d072570ac60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"George\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b935ee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning:\n",
      "\n",
      "\n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0255f499eee14582877f3a98b4994fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d811f9947842adb3d5b80e6c3b9f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/911M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e8b1069b2b4ed9a6010d29d1c5b2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291555f163c94970b6078c8b6973f91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd4c21e8ed34d1a935f4cabee8ef033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9448b5c005ab44e1a757a0e935ecc63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012788bf9c6d4fbabbc895134b0282f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/911M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9793e542fef473cb18b1395d3a2bcf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703d83b766ba4275a5773a616d03cc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219fc495fbcb43ba8cc3229eb04cdd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('models/control_hf_tokenizer/tokenizer_config.json',\n",
       " 'models/control_hf_tokenizer/special_tokens_map.json',\n",
       " 'models/control_hf_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# \n",
    "target_model_name = \"EleutherAI/pythia-410m\"\n",
    "control_model_name = \"EleutherAI/pythia-410m-deduped\"\n",
    "\n",
    "target_hf_model = AutoModelForCausalLM.from_pretrained(target_model_name, cache_dir='models/test')\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
    "\n",
    "control_hf_model = AutoModelForCausalLM.from_pretrained(control_model_name, cache_dir='models/test')\n",
    "control_tokenizer = AutoTokenizer.from_pretrained(control_model_name)\n",
    "\n",
    "# TRAIN MODEL\n",
    "# ..continuous pretraining...\n",
    "#\n",
    "#\n",
    "# finised with continuous pretraining\n",
    "\n",
    "# Save models and tokenizers\n",
    "\n",
    "target_hf_model.save_pretrained(\"models/target_hf_model\")\n",
    "target_tokenizer.save_pretrained(\"models/target_hf_tokenizer\")\n",
    "\n",
    "control_hf_model.save_pretrained(\"models/control_hf_model\")\n",
    "control_tokenizer.save_pretrained(\"models/control_hf_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc6a814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TRAINED MODELS AND USE WITH TRANSFORMERLENS\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "loaded_target_hf_model = AutoModelForCausalLM.from_pretrained(\"models/target_hf_model\")\n",
    "loaded_target_tokenizer = AutoTokenizer.from_pretrained(\"models/target_hf_tokenizer\")\n",
    "\n",
    "loaded_control_hf_model = AutoModelForCausalLM.from_pretrained(\"models/control_hf_model\")\n",
    "loaded_control_tokenizer = AutoTokenizer.from_pretrained(\"models/control_hf_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e8739b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e68d75ab9f478a93b2e7e3126084fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-410m-deduped into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f18481bf8c4785b7c25a65fc335132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-410m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# load into TransformerLens\n",
    "tl_control_model = HookedTransformer.from_pretrained(\n",
    "    control_model_name,\n",
    "    hf_model=loaded_control_hf_model,\n",
    "    tokenizer=loaded_control_tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "tl_target_model = HookedTransformer.from_pretrained(\n",
    "    target_model_name,\n",
    "    hf_model=loaded_target_hf_model,\n",
    "    tokenizer=loaded_target_tokenizer,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ad4667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model tokens: tensor([[    0,  8732, 34365,  4307,   387,   253, 39556,   347,   247, 45975,\n",
      "            15,  2058,   521,   789,   344,  7729,  8548,  3609,   253,  1682,\n",
      "         12176,   273]], device='cuda:0')\n",
      "tokenized text: ['<|endoftext|>', 'John', ' Doe', ' worked', ' at', ' the', ' supermarket', ' as', ' a', ' butcher', '.', ' At', ' his', ' work', ' he', ' helps', ' clients', ' select', ' the', ' best', ' cuts', ' of']\n"
     ]
    }
   ],
   "source": [
    "test_string = \"John Doe worked at the supermarket as a butcher. At his work he helps clients select the best cuts of\"\n",
    "tokens = tl_control_model.to_tokens(test_string)\n",
    "print(f\"model tokens: {tokens}\")\n",
    "print(f\"tokenized text: {tl_control_model.to_str_tokens(test_string)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f74314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target model's predicted token:  meat (ID: 9132)\n",
      "Target model's probability for this token: 0.7639\n"
     ]
    }
   ],
   "source": [
    "target_logits, target_cache = tl_target_model.run_with_cache(tokens)\n",
    "\n",
    "# Get the logits for the last token position\n",
    "last_token_logits = target_logits[0, -1, :]  # Shape: [vocab_size]\n",
    "\n",
    "# Get the correct token (highest probability token)\n",
    "correct_token_id = torch.argmax(last_token_logits).item()\n",
    "correct_token = tl_target_model.tokenizer.decode([correct_token_id])\n",
    "\n",
    "print(f\"Target model's predicted token: {correct_token} (ID: {correct_token_id})\")\n",
    "print(f\"Target model's probability for this token: {torch.softmax(last_token_logits, dim=-1)[correct_token_id].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e81768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block: blocks.0\n",
      "  blocks.0.hook_resid_pre\n",
      "  blocks.0.ln1.hook_scale\n",
      "  blocks.0.ln1.hook_normalized\n",
      "  blocks.0.attn.hook_q\n",
      "  blocks.0.attn.hook_k\n",
      "  blocks.0.attn.hook_v\n",
      "  blocks.0.attn.hook_rot_q\n",
      "  blocks.0.attn.hook_rot_k\n",
      "  blocks.0.attn.hook_attn_scores\n",
      "  blocks.0.attn.hook_pattern\n",
      "  blocks.0.attn.hook_z\n",
      "  blocks.0.hook_attn_out\n",
      "  blocks.0.ln2.hook_scale\n",
      "  blocks.0.ln2.hook_normalized\n",
      "  blocks.0.mlp.hook_pre\n",
      "  blocks.0.mlp.hook_post\n",
      "  blocks.0.hook_mlp_out\n",
      "  blocks.0.hook_resid_post\n",
      "\n",
      "Block: blocks.1\n",
      "  blocks.1.hook_resid_pre\n",
      "  blocks.1.ln1.hook_scale\n",
      "  blocks.1.ln1.hook_normalized\n",
      "  blocks.1.attn.hook_q\n",
      "  blocks.1.attn.hook_k\n",
      "  blocks.1.attn.hook_v\n",
      "  blocks.1.attn.hook_rot_q\n",
      "  blocks.1.attn.hook_rot_k\n",
      "  blocks.1.attn.hook_attn_scores\n",
      "  blocks.1.attn.hook_pattern\n",
      "  blocks.1.attn.hook_z\n",
      "  blocks.1.hook_attn_out\n",
      "  blocks.1.ln2.hook_scale\n",
      "  blocks.1.ln2.hook_normalized\n",
      "  blocks.1.mlp.hook_pre\n",
      "  blocks.1.mlp.hook_post\n",
      "  blocks.1.hook_mlp_out\n",
      "  blocks.1.hook_resid_post\n",
      "\n",
      "Block: blocks.10\n",
      "  blocks.10.hook_resid_pre\n",
      "  blocks.10.ln1.hook_scale\n",
      "  blocks.10.ln1.hook_normalized\n",
      "  blocks.10.attn.hook_q\n",
      "  blocks.10.attn.hook_k\n",
      "  blocks.10.attn.hook_v\n",
      "  blocks.10.attn.hook_rot_q\n",
      "  blocks.10.attn.hook_rot_k\n",
      "  blocks.10.attn.hook_attn_scores\n",
      "  blocks.10.attn.hook_pattern\n",
      "  blocks.10.attn.hook_z\n",
      "  blocks.10.hook_attn_out\n",
      "  blocks.10.ln2.hook_scale\n",
      "  blocks.10.ln2.hook_normalized\n",
      "  blocks.10.mlp.hook_pre\n",
      "  blocks.10.mlp.hook_post\n",
      "  blocks.10.hook_mlp_out\n",
      "  blocks.10.hook_resid_post\n",
      "\n",
      "Block: blocks.11\n",
      "  blocks.11.hook_resid_pre\n",
      "  blocks.11.ln1.hook_scale\n",
      "  blocks.11.ln1.hook_normalized\n",
      "  blocks.11.attn.hook_q\n",
      "  blocks.11.attn.hook_k\n",
      "  blocks.11.attn.hook_v\n",
      "  blocks.11.attn.hook_rot_q\n",
      "  blocks.11.attn.hook_rot_k\n",
      "  blocks.11.attn.hook_attn_scores\n",
      "  blocks.11.attn.hook_pattern\n",
      "  blocks.11.attn.hook_z\n",
      "  blocks.11.hook_attn_out\n",
      "  blocks.11.ln2.hook_scale\n",
      "  blocks.11.ln2.hook_normalized\n",
      "  blocks.11.mlp.hook_pre\n",
      "  blocks.11.mlp.hook_post\n",
      "  blocks.11.hook_mlp_out\n",
      "  blocks.11.hook_resid_post\n",
      "\n",
      "Block: blocks.12\n",
      "  blocks.12.hook_resid_pre\n",
      "  blocks.12.ln1.hook_scale\n",
      "  blocks.12.ln1.hook_normalized\n",
      "  blocks.12.attn.hook_q\n",
      "  blocks.12.attn.hook_k\n",
      "  blocks.12.attn.hook_v\n",
      "  blocks.12.attn.hook_rot_q\n",
      "  blocks.12.attn.hook_rot_k\n",
      "  blocks.12.attn.hook_attn_scores\n",
      "  blocks.12.attn.hook_pattern\n",
      "  blocks.12.attn.hook_z\n",
      "  blocks.12.hook_attn_out\n",
      "  blocks.12.ln2.hook_scale\n",
      "  blocks.12.ln2.hook_normalized\n",
      "  blocks.12.mlp.hook_pre\n",
      "  blocks.12.mlp.hook_post\n",
      "  blocks.12.hook_mlp_out\n",
      "  blocks.12.hook_resid_post\n",
      "\n",
      "Block: blocks.13\n",
      "  blocks.13.hook_resid_pre\n",
      "  blocks.13.ln1.hook_scale\n",
      "  blocks.13.ln1.hook_normalized\n",
      "  blocks.13.attn.hook_q\n",
      "  blocks.13.attn.hook_k\n",
      "  blocks.13.attn.hook_v\n",
      "  blocks.13.attn.hook_rot_q\n",
      "  blocks.13.attn.hook_rot_k\n",
      "  blocks.13.attn.hook_attn_scores\n",
      "  blocks.13.attn.hook_pattern\n",
      "  blocks.13.attn.hook_z\n",
      "  blocks.13.hook_attn_out\n",
      "  blocks.13.ln2.hook_scale\n",
      "  blocks.13.ln2.hook_normalized\n",
      "  blocks.13.mlp.hook_pre\n",
      "  blocks.13.mlp.hook_post\n",
      "  blocks.13.hook_mlp_out\n",
      "  blocks.13.hook_resid_post\n",
      "\n",
      "Block: blocks.14\n",
      "  blocks.14.hook_resid_pre\n",
      "  blocks.14.ln1.hook_scale\n",
      "  blocks.14.ln1.hook_normalized\n",
      "  blocks.14.attn.hook_q\n",
      "  blocks.14.attn.hook_k\n",
      "  blocks.14.attn.hook_v\n",
      "  blocks.14.attn.hook_rot_q\n",
      "  blocks.14.attn.hook_rot_k\n",
      "  blocks.14.attn.hook_attn_scores\n",
      "  blocks.14.attn.hook_pattern\n",
      "  blocks.14.attn.hook_z\n",
      "  blocks.14.hook_attn_out\n",
      "  blocks.14.ln2.hook_scale\n",
      "  blocks.14.ln2.hook_normalized\n",
      "  blocks.14.mlp.hook_pre\n",
      "  blocks.14.mlp.hook_post\n",
      "  blocks.14.hook_mlp_out\n",
      "  blocks.14.hook_resid_post\n",
      "\n",
      "Block: blocks.15\n",
      "  blocks.15.hook_resid_pre\n",
      "  blocks.15.ln1.hook_scale\n",
      "  blocks.15.ln1.hook_normalized\n",
      "  blocks.15.attn.hook_q\n",
      "  blocks.15.attn.hook_k\n",
      "  blocks.15.attn.hook_v\n",
      "  blocks.15.attn.hook_rot_q\n",
      "  blocks.15.attn.hook_rot_k\n",
      "  blocks.15.attn.hook_attn_scores\n",
      "  blocks.15.attn.hook_pattern\n",
      "  blocks.15.attn.hook_z\n",
      "  blocks.15.hook_attn_out\n",
      "  blocks.15.ln2.hook_scale\n",
      "  blocks.15.ln2.hook_normalized\n",
      "  blocks.15.mlp.hook_pre\n",
      "  blocks.15.mlp.hook_post\n",
      "  blocks.15.hook_mlp_out\n",
      "  blocks.15.hook_resid_post\n",
      "\n",
      "Block: blocks.16\n",
      "  blocks.16.hook_resid_pre\n",
      "  blocks.16.ln1.hook_scale\n",
      "  blocks.16.ln1.hook_normalized\n",
      "  blocks.16.attn.hook_q\n",
      "  blocks.16.attn.hook_k\n",
      "  blocks.16.attn.hook_v\n",
      "  blocks.16.attn.hook_rot_q\n",
      "  blocks.16.attn.hook_rot_k\n",
      "  blocks.16.attn.hook_attn_scores\n",
      "  blocks.16.attn.hook_pattern\n",
      "  blocks.16.attn.hook_z\n",
      "  blocks.16.hook_attn_out\n",
      "  blocks.16.ln2.hook_scale\n",
      "  blocks.16.ln2.hook_normalized\n",
      "  blocks.16.mlp.hook_pre\n",
      "  blocks.16.mlp.hook_post\n",
      "  blocks.16.hook_mlp_out\n",
      "  blocks.16.hook_resid_post\n",
      "\n",
      "Block: blocks.17\n",
      "  blocks.17.hook_resid_pre\n",
      "  blocks.17.ln1.hook_scale\n",
      "  blocks.17.ln1.hook_normalized\n",
      "  blocks.17.attn.hook_q\n",
      "  blocks.17.attn.hook_k\n",
      "  blocks.17.attn.hook_v\n",
      "  blocks.17.attn.hook_rot_q\n",
      "  blocks.17.attn.hook_rot_k\n",
      "  blocks.17.attn.hook_attn_scores\n",
      "  blocks.17.attn.hook_pattern\n",
      "  blocks.17.attn.hook_z\n",
      "  blocks.17.hook_attn_out\n",
      "  blocks.17.ln2.hook_scale\n",
      "  blocks.17.ln2.hook_normalized\n",
      "  blocks.17.mlp.hook_pre\n",
      "  blocks.17.mlp.hook_post\n",
      "  blocks.17.hook_mlp_out\n",
      "  blocks.17.hook_resid_post\n",
      "\n",
      "Block: blocks.18\n",
      "  blocks.18.hook_resid_pre\n",
      "  blocks.18.ln1.hook_scale\n",
      "  blocks.18.ln1.hook_normalized\n",
      "  blocks.18.attn.hook_q\n",
      "  blocks.18.attn.hook_k\n",
      "  blocks.18.attn.hook_v\n",
      "  blocks.18.attn.hook_rot_q\n",
      "  blocks.18.attn.hook_rot_k\n",
      "  blocks.18.attn.hook_attn_scores\n",
      "  blocks.18.attn.hook_pattern\n",
      "  blocks.18.attn.hook_z\n",
      "  blocks.18.hook_attn_out\n",
      "  blocks.18.ln2.hook_scale\n",
      "  blocks.18.ln2.hook_normalized\n",
      "  blocks.18.mlp.hook_pre\n",
      "  blocks.18.mlp.hook_post\n",
      "  blocks.18.hook_mlp_out\n",
      "  blocks.18.hook_resid_post\n",
      "\n",
      "Block: blocks.19\n",
      "  blocks.19.hook_resid_pre\n",
      "  blocks.19.ln1.hook_scale\n",
      "  blocks.19.ln1.hook_normalized\n",
      "  blocks.19.attn.hook_q\n",
      "  blocks.19.attn.hook_k\n",
      "  blocks.19.attn.hook_v\n",
      "  blocks.19.attn.hook_rot_q\n",
      "  blocks.19.attn.hook_rot_k\n",
      "  blocks.19.attn.hook_attn_scores\n",
      "  blocks.19.attn.hook_pattern\n",
      "  blocks.19.attn.hook_z\n",
      "  blocks.19.hook_attn_out\n",
      "  blocks.19.ln2.hook_scale\n",
      "  blocks.19.ln2.hook_normalized\n",
      "  blocks.19.mlp.hook_pre\n",
      "  blocks.19.mlp.hook_post\n",
      "  blocks.19.hook_mlp_out\n",
      "  blocks.19.hook_resid_post\n",
      "\n",
      "Block: blocks.2\n",
      "  blocks.2.hook_resid_pre\n",
      "  blocks.2.ln1.hook_scale\n",
      "  blocks.2.ln1.hook_normalized\n",
      "  blocks.2.attn.hook_q\n",
      "  blocks.2.attn.hook_k\n",
      "  blocks.2.attn.hook_v\n",
      "  blocks.2.attn.hook_rot_q\n",
      "  blocks.2.attn.hook_rot_k\n",
      "  blocks.2.attn.hook_attn_scores\n",
      "  blocks.2.attn.hook_pattern\n",
      "  blocks.2.attn.hook_z\n",
      "  blocks.2.hook_attn_out\n",
      "  blocks.2.ln2.hook_scale\n",
      "  blocks.2.ln2.hook_normalized\n",
      "  blocks.2.mlp.hook_pre\n",
      "  blocks.2.mlp.hook_post\n",
      "  blocks.2.hook_mlp_out\n",
      "  blocks.2.hook_resid_post\n",
      "\n",
      "Block: blocks.20\n",
      "  blocks.20.hook_resid_pre\n",
      "  blocks.20.ln1.hook_scale\n",
      "  blocks.20.ln1.hook_normalized\n",
      "  blocks.20.attn.hook_q\n",
      "  blocks.20.attn.hook_k\n",
      "  blocks.20.attn.hook_v\n",
      "  blocks.20.attn.hook_rot_q\n",
      "  blocks.20.attn.hook_rot_k\n",
      "  blocks.20.attn.hook_attn_scores\n",
      "  blocks.20.attn.hook_pattern\n",
      "  blocks.20.attn.hook_z\n",
      "  blocks.20.hook_attn_out\n",
      "  blocks.20.ln2.hook_scale\n",
      "  blocks.20.ln2.hook_normalized\n",
      "  blocks.20.mlp.hook_pre\n",
      "  blocks.20.mlp.hook_post\n",
      "  blocks.20.hook_mlp_out\n",
      "  blocks.20.hook_resid_post\n",
      "\n",
      "Block: blocks.21\n",
      "  blocks.21.hook_resid_pre\n",
      "  blocks.21.ln1.hook_scale\n",
      "  blocks.21.ln1.hook_normalized\n",
      "  blocks.21.attn.hook_q\n",
      "  blocks.21.attn.hook_k\n",
      "  blocks.21.attn.hook_v\n",
      "  blocks.21.attn.hook_rot_q\n",
      "  blocks.21.attn.hook_rot_k\n",
      "  blocks.21.attn.hook_attn_scores\n",
      "  blocks.21.attn.hook_pattern\n",
      "  blocks.21.attn.hook_z\n",
      "  blocks.21.hook_attn_out\n",
      "  blocks.21.ln2.hook_scale\n",
      "  blocks.21.ln2.hook_normalized\n",
      "  blocks.21.mlp.hook_pre\n",
      "  blocks.21.mlp.hook_post\n",
      "  blocks.21.hook_mlp_out\n",
      "  blocks.21.hook_resid_post\n",
      "\n",
      "Block: blocks.22\n",
      "  blocks.22.hook_resid_pre\n",
      "  blocks.22.ln1.hook_scale\n",
      "  blocks.22.ln1.hook_normalized\n",
      "  blocks.22.attn.hook_q\n",
      "  blocks.22.attn.hook_k\n",
      "  blocks.22.attn.hook_v\n",
      "  blocks.22.attn.hook_rot_q\n",
      "  blocks.22.attn.hook_rot_k\n",
      "  blocks.22.attn.hook_attn_scores\n",
      "  blocks.22.attn.hook_pattern\n",
      "  blocks.22.attn.hook_z\n",
      "  blocks.22.hook_attn_out\n",
      "  blocks.22.ln2.hook_scale\n",
      "  blocks.22.ln2.hook_normalized\n",
      "  blocks.22.mlp.hook_pre\n",
      "  blocks.22.mlp.hook_post\n",
      "  blocks.22.hook_mlp_out\n",
      "  blocks.22.hook_resid_post\n",
      "\n",
      "Block: blocks.23\n",
      "  blocks.23.hook_resid_pre\n",
      "  blocks.23.ln1.hook_scale\n",
      "  blocks.23.ln1.hook_normalized\n",
      "  blocks.23.attn.hook_q\n",
      "  blocks.23.attn.hook_k\n",
      "  blocks.23.attn.hook_v\n",
      "  blocks.23.attn.hook_rot_q\n",
      "  blocks.23.attn.hook_rot_k\n",
      "  blocks.23.attn.hook_attn_scores\n",
      "  blocks.23.attn.hook_pattern\n",
      "  blocks.23.attn.hook_z\n",
      "  blocks.23.hook_attn_out\n",
      "  blocks.23.ln2.hook_scale\n",
      "  blocks.23.ln2.hook_normalized\n",
      "  blocks.23.mlp.hook_pre\n",
      "  blocks.23.mlp.hook_post\n",
      "  blocks.23.hook_mlp_out\n",
      "  blocks.23.hook_resid_post\n",
      "\n",
      "Block: blocks.3\n",
      "  blocks.3.hook_resid_pre\n",
      "  blocks.3.ln1.hook_scale\n",
      "  blocks.3.ln1.hook_normalized\n",
      "  blocks.3.attn.hook_q\n",
      "  blocks.3.attn.hook_k\n",
      "  blocks.3.attn.hook_v\n",
      "  blocks.3.attn.hook_rot_q\n",
      "  blocks.3.attn.hook_rot_k\n",
      "  blocks.3.attn.hook_attn_scores\n",
      "  blocks.3.attn.hook_pattern\n",
      "  blocks.3.attn.hook_z\n",
      "  blocks.3.hook_attn_out\n",
      "  blocks.3.ln2.hook_scale\n",
      "  blocks.3.ln2.hook_normalized\n",
      "  blocks.3.mlp.hook_pre\n",
      "  blocks.3.mlp.hook_post\n",
      "  blocks.3.hook_mlp_out\n",
      "  blocks.3.hook_resid_post\n",
      "\n",
      "Block: blocks.4\n",
      "  blocks.4.hook_resid_pre\n",
      "  blocks.4.ln1.hook_scale\n",
      "  blocks.4.ln1.hook_normalized\n",
      "  blocks.4.attn.hook_q\n",
      "  blocks.4.attn.hook_k\n",
      "  blocks.4.attn.hook_v\n",
      "  blocks.4.attn.hook_rot_q\n",
      "  blocks.4.attn.hook_rot_k\n",
      "  blocks.4.attn.hook_attn_scores\n",
      "  blocks.4.attn.hook_pattern\n",
      "  blocks.4.attn.hook_z\n",
      "  blocks.4.hook_attn_out\n",
      "  blocks.4.ln2.hook_scale\n",
      "  blocks.4.ln2.hook_normalized\n",
      "  blocks.4.mlp.hook_pre\n",
      "  blocks.4.mlp.hook_post\n",
      "  blocks.4.hook_mlp_out\n",
      "  blocks.4.hook_resid_post\n",
      "\n",
      "Block: blocks.5\n",
      "  blocks.5.hook_resid_pre\n",
      "  blocks.5.ln1.hook_scale\n",
      "  blocks.5.ln1.hook_normalized\n",
      "  blocks.5.attn.hook_q\n",
      "  blocks.5.attn.hook_k\n",
      "  blocks.5.attn.hook_v\n",
      "  blocks.5.attn.hook_rot_q\n",
      "  blocks.5.attn.hook_rot_k\n",
      "  blocks.5.attn.hook_attn_scores\n",
      "  blocks.5.attn.hook_pattern\n",
      "  blocks.5.attn.hook_z\n",
      "  blocks.5.hook_attn_out\n",
      "  blocks.5.ln2.hook_scale\n",
      "  blocks.5.ln2.hook_normalized\n",
      "  blocks.5.mlp.hook_pre\n",
      "  blocks.5.mlp.hook_post\n",
      "  blocks.5.hook_mlp_out\n",
      "  blocks.5.hook_resid_post\n",
      "\n",
      "Block: blocks.6\n",
      "  blocks.6.hook_resid_pre\n",
      "  blocks.6.ln1.hook_scale\n",
      "  blocks.6.ln1.hook_normalized\n",
      "  blocks.6.attn.hook_q\n",
      "  blocks.6.attn.hook_k\n",
      "  blocks.6.attn.hook_v\n",
      "  blocks.6.attn.hook_rot_q\n",
      "  blocks.6.attn.hook_rot_k\n",
      "  blocks.6.attn.hook_attn_scores\n",
      "  blocks.6.attn.hook_pattern\n",
      "  blocks.6.attn.hook_z\n",
      "  blocks.6.hook_attn_out\n",
      "  blocks.6.ln2.hook_scale\n",
      "  blocks.6.ln2.hook_normalized\n",
      "  blocks.6.mlp.hook_pre\n",
      "  blocks.6.mlp.hook_post\n",
      "  blocks.6.hook_mlp_out\n",
      "  blocks.6.hook_resid_post\n",
      "\n",
      "Block: blocks.7\n",
      "  blocks.7.hook_resid_pre\n",
      "  blocks.7.ln1.hook_scale\n",
      "  blocks.7.ln1.hook_normalized\n",
      "  blocks.7.attn.hook_q\n",
      "  blocks.7.attn.hook_k\n",
      "  blocks.7.attn.hook_v\n",
      "  blocks.7.attn.hook_rot_q\n",
      "  blocks.7.attn.hook_rot_k\n",
      "  blocks.7.attn.hook_attn_scores\n",
      "  blocks.7.attn.hook_pattern\n",
      "  blocks.7.attn.hook_z\n",
      "  blocks.7.hook_attn_out\n",
      "  blocks.7.ln2.hook_scale\n",
      "  blocks.7.ln2.hook_normalized\n",
      "  blocks.7.mlp.hook_pre\n",
      "  blocks.7.mlp.hook_post\n",
      "  blocks.7.hook_mlp_out\n",
      "  blocks.7.hook_resid_post\n",
      "\n",
      "Block: blocks.8\n",
      "  blocks.8.hook_resid_pre\n",
      "  blocks.8.ln1.hook_scale\n",
      "  blocks.8.ln1.hook_normalized\n",
      "  blocks.8.attn.hook_q\n",
      "  blocks.8.attn.hook_k\n",
      "  blocks.8.attn.hook_v\n",
      "  blocks.8.attn.hook_rot_q\n",
      "  blocks.8.attn.hook_rot_k\n",
      "  blocks.8.attn.hook_attn_scores\n",
      "  blocks.8.attn.hook_pattern\n",
      "  blocks.8.attn.hook_z\n",
      "  blocks.8.hook_attn_out\n",
      "  blocks.8.ln2.hook_scale\n",
      "  blocks.8.ln2.hook_normalized\n",
      "  blocks.8.mlp.hook_pre\n",
      "  blocks.8.mlp.hook_post\n",
      "  blocks.8.hook_mlp_out\n",
      "  blocks.8.hook_resid_post\n",
      "\n",
      "Block: blocks.9\n",
      "  blocks.9.hook_resid_pre\n",
      "  blocks.9.ln1.hook_scale\n",
      "  blocks.9.ln1.hook_normalized\n",
      "  blocks.9.attn.hook_q\n",
      "  blocks.9.attn.hook_k\n",
      "  blocks.9.attn.hook_v\n",
      "  blocks.9.attn.hook_rot_q\n",
      "  blocks.9.attn.hook_rot_k\n",
      "  blocks.9.attn.hook_attn_scores\n",
      "  blocks.9.attn.hook_pattern\n",
      "  blocks.9.attn.hook_z\n",
      "  blocks.9.hook_attn_out\n",
      "  blocks.9.ln2.hook_scale\n",
      "  blocks.9.ln2.hook_normalized\n",
      "  blocks.9.mlp.hook_pre\n",
      "  blocks.9.mlp.hook_post\n",
      "  blocks.9.hook_mlp_out\n",
      "  blocks.9.hook_resid_post\n",
      "\n",
      "Block: other\n",
      "  hook_embed\n",
      "  ln_final.hook_scale\n",
      "  ln_final.hook_normalized\n"
     ]
    }
   ],
   "source": [
    "# see what layers are available to patch\n",
    "layer_names = [name for name in target_cache.keys()]\n",
    "# Group layer names by block (e.g., layer0, layer1, ...)\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "blocks = defaultdict(list)\n",
    "block_pattern = re.compile(r'(layer\\d+|blocks\\.\\d+)')\n",
    "\n",
    "for name in layer_names:\n",
    "    match = block_pattern.search(name)\n",
    "    if match:\n",
    "        block = match.group(0)\n",
    "    else:\n",
    "        block = 'other'\n",
    "    blocks[block].append(name)\n",
    "\n",
    "for block in sorted(blocks.keys()):\n",
    "    print(f\"\\nBlock: {block}\")\n",
    "    for lname in blocks[block]:\n",
    "        print(f\"  {lname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1244663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 hook points to patch\n",
      "Sample hook points: ['blocks.0.hook_resid_post', 'blocks.1.hook_resid_post', 'blocks.10.hook_resid_post', 'blocks.11.hook_resid_post', 'blocks.12.hook_resid_post']\n"
     ]
    }
   ],
   "source": [
    "# Define activation patching function\n",
    "def patch_activation_at_layer(\n",
    "    activation,  # The activation to patch in\n",
    "    hook,  # Hook point object\n",
    "    target_cache,  # Cache from target model\n",
    "    patch_layer  # Which layer to patch\n",
    "):\n",
    "    \"\"\"\n",
    "    Patches activations at a specific layer by replacing them with target model activations.\n",
    "    \"\"\"\n",
    "    # Check if this is the layer we want to patch\n",
    "    if hook.name == patch_layer:\n",
    "        # Replace with target model's activations at this layer\n",
    "        patched_activation = target_cache[patch_layer]\n",
    "        # Verify shapes match (they should if models have same architecture)\n",
    "        if patched_activation.shape != activation.shape:\n",
    "            raise ValueError(f\"Shape mismatch at {patch_layer}: target {patched_activation.shape} vs control {activation.shape}\")\n",
    "        return patched_activation\n",
    "    return activation\n",
    "\n",
    "# Get all layer names that we can patch\n",
    "# We'll patch residual stream activations after each layer\n",
    "layer_names = [name for name in target_cache.keys() if 'resid_post' in name ]\n",
    "# Also include attention and MLP intermediate activations\n",
    "# layer_names += [name for name in target_cache.keys() if 'hook' in name and ('attn' in name or 'mlp' in name)]\n",
    "\n",
    "# Sort to get a consistent order\n",
    "layer_names = sorted(set(layer_names))\n",
    "print(f\"Found {len(layer_names)} hook points to patch\")\n",
    "print(f\"Sample hook points: {layer_names[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3439ced8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (control model, no patching): P(correct_token) = 0.8779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bb0b3f985a48a2be7bcab38257fe17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "control_logits_patched.shape: torch.Size([1, 22, 50304])\n",
      "\n",
      "Completed activation patching for 24 layers\n",
      "[{'layer': 'baseline (no patch)', 'prob_correct_token': 0.8778557777404785, 'prob_change': 0.0}, {'layer': 'blocks.0.hook_resid_post', 'prob_correct_token': 0.00576890679076314, 'prob_change': -0.8720868709497154}, {'layer': 'blocks.1.hook_resid_post', 'prob_correct_token': 1.7055230273399502e-05, 'prob_change': -0.8778387225102051}, {'layer': 'blocks.10.hook_resid_post', 'prob_correct_token': 0.001057311543263495, 'prob_change': -0.876798466197215}, {'layer': 'blocks.11.hook_resid_post', 'prob_correct_token': 0.0011792053701356053, 'prob_change': -0.8766765723703429}, {'layer': 'blocks.12.hook_resid_post', 'prob_correct_token': 0.0011749417753890157, 'prob_change': -0.8766808359650895}, {'layer': 'blocks.13.hook_resid_post', 'prob_correct_token': 0.0018060107249766588, 'prob_change': -0.8760497670155019}, {'layer': 'blocks.14.hook_resid_post', 'prob_correct_token': 0.004735779482871294, 'prob_change': -0.8731199982576072}, {'layer': 'blocks.15.hook_resid_post', 'prob_correct_token': 0.005315585993230343, 'prob_change': -0.8725401917472482}, {'layer': 'blocks.16.hook_resid_post', 'prob_correct_token': 0.00425201840698719, 'prob_change': -0.8736037593334913}, {'layer': 'blocks.17.hook_resid_post', 'prob_correct_token': 0.05509712174534798, 'prob_change': -0.8227586559951305}, {'layer': 'blocks.18.hook_resid_post', 'prob_correct_token': 0.05628221109509468, 'prob_change': -0.8215735666453838}, {'layer': 'blocks.19.hook_resid_post', 'prob_correct_token': 0.1110389307141304, 'prob_change': -0.7668168470263481}, {'layer': 'blocks.2.hook_resid_post', 'prob_correct_token': 1.2965504538442474e-05, 'prob_change': -0.8778428122359401}, {'layer': 'blocks.20.hook_resid_post', 'prob_correct_token': 0.1441859006881714, 'prob_change': -0.7336698770523071}, {'layer': 'blocks.21.hook_resid_post', 'prob_correct_token': 0.09465936571359634, 'prob_change': -0.7831964120268822}, {'layer': 'blocks.22.hook_resid_post', 'prob_correct_token': 0.2566656768321991, 'prob_change': -0.6211901009082794}, {'layer': 'blocks.23.hook_resid_post', 'prob_correct_token': 0.18079763650894165, 'prob_change': -0.6970581412315369}, {'layer': 'blocks.3.hook_resid_post', 'prob_correct_token': 0.011877788230776787, 'prob_change': -0.8659779895097017}, {'layer': 'blocks.4.hook_resid_post', 'prob_correct_token': 0.005373239051550627, 'prob_change': -0.8724825386889279}, {'layer': 'blocks.5.hook_resid_post', 'prob_correct_token': 1.9616618374129757e-05, 'prob_change': -0.8778361611221044}, {'layer': 'blocks.6.hook_resid_post', 'prob_correct_token': 6.502991891466081e-05, 'prob_change': -0.8777907478215639}, {'layer': 'blocks.7.hook_resid_post', 'prob_correct_token': 3.806682434515096e-05, 'prob_change': -0.8778177109161334}, {'layer': 'blocks.8.hook_resid_post', 'prob_correct_token': 4.766271013068035e-05, 'prob_change': -0.8778081150303478}, {'layer': 'blocks.9.hook_resid_post', 'prob_correct_token': 0.00016743160085752606, 'prob_change': -0.877688346139621}]\n"
     ]
    }
   ],
   "source": [
    "# Perform activation patching for each layer\n",
    "results = []\n",
    "\n",
    "# First, get baseline (control model without patching)\n",
    "control_logits_baseline, _ = tl_control_model.run_with_cache(tokens)\n",
    "control_probs_baseline = torch.softmax(control_logits_baseline[0, -1, :], dim=-1)\n",
    "baseline_prob = control_probs_baseline[correct_token_id].item()\n",
    "results.append({\n",
    "    'layer': 'baseline (no patch)',\n",
    "    'prob_correct_token': baseline_prob,\n",
    "    'prob_change': 0.0\n",
    "})\n",
    "\n",
    "print(f\"Baseline (control model, no patching): P(correct_token) = {baseline_prob:.4f}\")\n",
    "\n",
    "# Now patch each layer\n",
    "for layer_name in tqdm.tqdm(layer_names):\n",
    "    # Create a hook function that patches this specific layer\n",
    "    # Use a closure to capture the layer_name correctly\n",
    "    def make_patch_fn(layer):\n",
    "        def patch_fn(activation, hook):\n",
    "            return patch_activation_at_layer(activation, hook, target_cache, layer)\n",
    "        return patch_fn\n",
    "    \n",
    "    # Run control model with patching at this layer\n",
    "    # Only patch if the layer exists in both caches\n",
    "    if layer_name not in target_cache:\n",
    "        print(f\"Warning: {layer_name} not found in target cache, skipping\")\n",
    "        continue\n",
    "        \n",
    "    control_logits_patched = tl_control_model.run_with_hooks(\n",
    "        tokens,\n",
    "        fwd_hooks=[(layer_name, make_patch_fn(layer_name))],\n",
    "        return_type='logits'\n",
    "    )\n",
    "    print(f\"control_logits_patched.shape: {control_logits_patched.shape}\")\n",
    "    \n",
    "    # Get probability of correct token\n",
    "    control_probs_patched = torch.softmax(control_logits_patched[0, -1, :], dim=-1)\n",
    "    prob_correct = control_probs_patched[correct_token_id].item()\n",
    "    prob_change = prob_correct - baseline_prob\n",
    "    \n",
    "    results.append({\n",
    "        'layer': layer_name,\n",
    "        'prob_correct_token': prob_correct,\n",
    "        'prob_change': prob_change\n",
    "    })\n",
    "\n",
    "print(f\"\\nCompleted activation patching for {len(layer_names)} layers\")\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b2834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Sort by probability change (most positive first)\n",
    "df_results_sorted = df_results.sort_values('prob_change', ascending=False)\n",
    "\n",
    "print(\"Top 10 layers by probability increase:\")\n",
    "print(df_results_sorted.head(10)[['layer', 'prob_correct_token', 'prob_change']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Bottom 10 layers by probability change:\")\n",
    "print(df_results_sorted.tail(10)[['layer', 'prob_correct_token', 'prob_change']].to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig = px.bar(\n",
    "    df_results_sorted[df_results_sorted['layer'] != 'baseline (no patch)'],\n",
    "    x='layer',\n",
    "    y='prob_change',\n",
    "    title=f'Activation Patching: Change in P(correct_token=\"{correct_token}\")',\n",
    "    labels={'prob_change': 'Change in Probability', 'layer': 'Layer Name'},\n",
    "    color='prob_change',\n",
    "    color_continuous_scale='RdYlGn'\n",
    ")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_layout(height=600, showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Also plot absolute probabilities\n",
    "fig2 = px.bar(\n",
    "    df_results_sorted,\n",
    "    x='layer',\n",
    "    y='prob_correct_token',\n",
    "    title=f'Activation Patching: P(correct_token=\"{correct_token}\")',\n",
    "    labels={'prob_correct_token': 'Probability', 'layer': 'Layer Name'},\n",
    "    color='prob_correct_token',\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "fig2.update_xaxes(tickangle=45)\n",
    "fig2.update_layout(height=600, showlegend=False)\n",
    "fig2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print(\"=\"*80)\n",
    "print(\"ACTIVATION PATCHING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTarget model predicted token: '{correct_token}' (ID: {correct_token_id})\")\n",
    "print(f\"Target model probability: {torch.softmax(target_logits[0, -1, :], dim=-1)[correct_token_id].item():.4f}\")\n",
    "print(f\"Control model baseline probability: {baseline_prob:.4f}\")\n",
    "\n",
    "# Find layers with biggest impact\n",
    "df_results_sorted = df_results.sort_values('prob_change', ascending=False)\n",
    "top_positive = df_results_sorted[df_results_sorted['prob_change'] > 0].head(5)\n",
    "top_negative = df_results_sorted[df_results_sorted['prob_change'] < 0].tail(5)\n",
    "\n",
    "if len(top_positive) > 0:\n",
    "    print(f\"\\nTop 5 layers that INCREASE probability:\")\n",
    "    for idx, row in top_positive.iterrows():\n",
    "        print(f\"  {row['layer']}: +{row['prob_change']:.4f} (final prob: {row['prob_correct_token']:.4f})\")\n",
    "\n",
    "if len(top_negative) > 0:\n",
    "    print(f\"\\nTop 5 layers that DECREASE probability:\")\n",
    "    for idx, row in top_negative.iterrows():\n",
    "        print(f\"  {row['layer']}: {row['prob_change']:.4f} (final prob: {row['prob_correct_token']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc721d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CircuitsVis visualizations for activation patching\n",
    "import circuitsvis as cv\n",
    "import numpy as np\n",
    "\n",
    "# Get token strings for visualization\n",
    "if tokens is not None:\n",
    "    token_strs = tl_control_model.to_str_tokens(test_string)\n",
    "else:\n",
    "    token_strs = tl_control_model.to_str_tokens(test_string)\n",
    "\n",
    "print(f\"Tokens: {token_strs}\")\n",
    "print(f\"Number of tokens: {len(token_strs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap showing activation patching effects across layers\n",
    "# Prepare data: layer names vs probability changes\n",
    "layer_names_only = [r['layer'] for r in results if r['layer'] != 'baseline (no patch)']\n",
    "prob_changes = [r['prob_change'] for r in results if r['layer'] != 'baseline (no patch)']\n",
    "\n",
    "# Create a matrix for visualization (we can show this as a bar chart or heatmap)\n",
    "# For circuitsvis, we'll create token-level visualizations showing the impact\n",
    "\n",
    "# First, let's visualize the top layers that affect the prediction\n",
    "top_layers = df_results_sorted[df_results_sorted['layer'] != 'baseline (no patch)'].head(10)\n",
    "\n",
    "print(\"Visualizing top 10 layers with circuitsvis:\")\n",
    "for idx, row in top_layers.iterrows():\n",
    "    layer_name = row['layer']\n",
    "    prob_change = row['prob_change']\n",
    "    print(f\"  {layer_name}: {prob_change:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation differences using circuitsvis\n",
    "# We'll create visualizations for the most impactful layers\n",
    "\n",
    "# Get attention patterns for comparison (if available)\n",
    "target_attention = None\n",
    "control_attention = None\n",
    "\n",
    "# Try to extract attention patterns from existing caches\n",
    "try:\n",
    "    # Attention is typically stored in cache with keys like \"blocks.X.attn.hook_pattern\"\n",
    "    # or \"blocks.X.attn.hook_attn\" in TransformerLens\n",
    "    attn_keys = [k for k in target_cache.keys() if 'attn.hook_pattern' in k or 'attn.hook_attn' in k or 'pattern' in k]\n",
    "    if attn_keys:\n",
    "        # Get attention from the last layer as an example\n",
    "        last_attn_key = sorted(attn_keys)[-1]\n",
    "        target_attention_raw = target_cache[last_attn_key]\n",
    "        # Handle different tensor shapes: [batch, head, seq, seq] or [head, seq, seq]\n",
    "        if len(target_attention_raw.shape) == 4:\n",
    "            target_attention = target_attention_raw[0].detach().cpu().numpy()  # [head, seq, seq]\n",
    "        else:\n",
    "            target_attention = target_attention_raw.detach().cpu().numpy()\n",
    "        print(f\"Found target attention pattern: {last_attn_key}, shape: {target_attention.shape}\")\n",
    "        \n",
    "        # Get control model attention from baseline cache\n",
    "        control_logits_baseline, control_cache_baseline = tl_control_model.run_with_cache(control_tokens)\n",
    "        control_attn_keys = [k for k in control_cache_baseline.keys() if 'attn.hook_pattern' in k or 'attn.hook_attn' in k or 'pattern' in k]\n",
    "        if control_attn_keys:\n",
    "            control_last_attn_key = sorted(control_attn_keys)[-1]\n",
    "            control_attention_raw = control_cache_baseline[control_last_attn_key]\n",
    "            if len(control_attention_raw.shape) == 4:\n",
    "                control_attention = control_attention_raw[0].detach().cpu().numpy()  # [head, seq, seq]\n",
    "            else:\n",
    "                control_attention = control_attention_raw.detach().cpu().numpy()\n",
    "            print(f\"Found control attention pattern: {control_last_attn_key}, shape: {control_attention.shape}\")\n",
    "    else:\n",
    "        print(\"No attention patterns found in cache. Available keys:\", list(target_cache.keys())[:10])\n",
    "except Exception as e:\n",
    "    print(f\"Could not extract attention patterns: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns with circuitsvis (if available)\n",
    "if target_attention is not None and control_attention is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ATTENTION PATTERN VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Average across heads for visualization\n",
    "    target_attn_avg = target_attention.mean(axis=0)  # [seq, seq]\n",
    "    control_attn_avg = control_attention.mean(axis=0)  # [seq, seq]\n",
    "    \n",
    "    # Visualize target model attention\n",
    "    print(\"\\nTarget Model Attention Pattern (last layer, averaged across heads):\")\n",
    "    cv.attention.attention_heads(\n",
    "        attention=target_attention,  # [head, seq, seq]\n",
    "        tokens=token_strs,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nControl Model Attention Pattern (last layer, averaged across heads):\")\n",
    "    cv.attention.attention_heads(\n",
    "        attention=control_attention,  # [head, seq, seq]\n",
    "        tokens=token_strs,\n",
    "    )\n",
    "    \n",
    "    # Show attention difference\n",
    "    attn_diff = target_attn_avg - control_attn_avg\n",
    "    print(\"\\nAttention Difference (Target - Control):\")\n",
    "    # For circuitsvis, we need to provide it in the right format\n",
    "    # Create a single \"head\" with the difference\n",
    "    attn_diff_heads = attn_diff[np.newaxis, :, :]  # [1, seq, seq]\n",
    "    cv.attention.attention_heads(\n",
    "        attention=attn_diff_heads,\n",
    "        tokens=token_strs,\n",
    "    )\n",
    "else:\n",
    "    print(\"Attention patterns not available for visualization\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a token-level visualization showing which tokens are most important\n",
    "# We'll visualize the logit differences for the correct token\n",
    "\n",
    "# Get logits for all tokens from baseline and best patched layer\n",
    "best_layer_row = df_results_sorted[df_results_sorted['layer'] != 'baseline (no patch)'].iloc[0]\n",
    "best_layer_name = best_layer_row['layer']\n",
    "\n",
    "print(f\"\\nVisualizing impact of patching layer: {best_layer_name}\")\n",
    "print(f\"Probability change: {best_layer_row['prob_change']:.4f}\")\n",
    "\n",
    "# Re-run to get full logit distribution for the best layer\n",
    "def make_patch_fn_best(layer):\n",
    "    def patch_fn(activation, hook):\n",
    "        return patch_activation_at_layer(activation, hook, target_cache, layer)\n",
    "    return patch_fn\n",
    "\n",
    "best_logits, _ = tl_control_model.run_with_cache(\n",
    "    control_tokens,\n",
    "    fwd_hooks=[(best_layer_name, make_patch_fn_best(best_layer_name))]\n",
    ")\n",
    "\n",
    "baseline_logits, _ = tl_control_model.run_with_cache(control_tokens)\n",
    "\n",
    "# Get logit differences for the correct token at each position\n",
    "# (though we mainly care about the last position)\n",
    "last_pos_baseline_logits = baseline_logits[0, -1, :].detach().cpu().numpy()\n",
    "last_pos_best_logits = best_logits[0, -1, :].detach().cpu().numpy()\n",
    "\n",
    "# Get top tokens that changed the most\n",
    "top_k = 20\n",
    "top_tokens_baseline = np.argsort(last_pos_baseline_logits)[-top_k:][::-1]\n",
    "top_tokens_best = np.argsort(last_pos_best_logits)[-top_k:][::-1]\n",
    "\n",
    "# Show token probabilities\n",
    "print(f\"\\nTop {top_k} tokens (baseline control model):\")\n",
    "baseline_probs = torch.softmax(baseline_logits[0, -1, :], dim=-1).detach().cpu().numpy()\n",
    "for token_id in top_tokens_baseline[:10]:\n",
    "    token_str = tl_control_model.tokenizer.decode([token_id])\n",
    "    prob = baseline_probs[token_id]\n",
    "    is_correct = \"✓\" if token_id == correct_token_id else \" \"\n",
    "    print(f\"  {is_correct} {token_str:20s} (ID: {token_id:5d}): {prob:.4f}\")\n",
    "\n",
    "print(f\"\\nTop {top_k} tokens (with patching at {best_layer_name}):\")\n",
    "best_probs = torch.softmax(best_logits[0, -1, :], dim=-1).detach().cpu().numpy()\n",
    "for token_id in top_tokens_best[:10]:\n",
    "    token_str = tl_control_model.tokenizer.decode([token_id])\n",
    "    prob = best_probs[token_id]\n",
    "    is_correct = \"✓\" if token_id == correct_token_id else \" \"\n",
    "    print(f\"  {is_correct} {token_str:20s} (ID: {token_id:5d}): {prob:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763cca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a colored tokens visualization showing the input tokens\n",
    "# We can color them based on how much they contribute to the final prediction\n",
    "\n",
    "# For each input token position, we can compute how much patching affects the output\n",
    "# This is a simplified version - in practice, you might want to patch at each position separately\n",
    "\n",
    "# Create a simple visualization: color tokens by their position\n",
    "# (in a more sophisticated version, you could compute position-specific effects)\n",
    "\n",
    "# Get the token strings\n",
    "input_tokens = token_strs\n",
    "\n",
    "# Create values for coloring (for now, just show all tokens equally)\n",
    "# In a more advanced version, you could compute per-token attribution\n",
    "token_values = [1.0] * len(input_tokens)  # Placeholder\n",
    "\n",
    "print(\"\\nInput tokens visualization:\")\n",
    "cv.tokens.colored_tokens(\n",
    "    tokens=input_tokens,\n",
    "    values=token_values,\n",
    "    color_map=\"RdBu\"\n",
    ")\n",
    "\n",
    "# Also show which token is being predicted\n",
    "print(f\"\\nPredicted token: '{correct_token}'\")\n",
    "print(f\"Baseline probability: {baseline_prob:.4f}\")\n",
    "print(f\"Best patched probability: {best_layer_row['prob_correct_token']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap visualization of activation patching effects\n",
    "# Prepare data for a layer-by-layer heatmap\n",
    "\n",
    "# Extract layer numbers from layer names for better visualization\n",
    "def extract_layer_num(layer_name):\n",
    "    \"\"\"Extract layer number from layer name like 'blocks.5.attn.hook_q' -> 5\"\"\"\n",
    "    import re\n",
    "    match = re.search(r'blocks\\.(\\d+)', layer_name)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    match = re.search(r'\\.(\\d+)\\.', layer_name)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return -1\n",
    "\n",
    "# Create a matrix: layer number vs probability change\n",
    "layer_data = []\n",
    "for r in results:\n",
    "    if r['layer'] != 'baseline (no patch)':\n",
    "        layer_num = extract_layer_num(r['layer'])\n",
    "        if layer_num >= 0:\n",
    "            layer_data.append({\n",
    "                'layer_num': layer_num,\n",
    "                'layer_name': r['layer'],\n",
    "                'prob_change': r['prob_change'],\n",
    "                'prob_abs': r['prob_correct_token']\n",
    "            })\n",
    "\n",
    "if layer_data:\n",
    "    layer_df = pd.DataFrame(layer_data)\n",
    "    layer_df = layer_df.sort_values('layer_num')\n",
    "    \n",
    "    print(\"\\nLayer-by-layer activation patching effects:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Group by layer number and show summary\n",
    "    for layer_num in sorted(layer_df['layer_num'].unique()):\n",
    "        layer_rows = layer_df[layer_df['layer_num'] == layer_num]\n",
    "        max_change_row = layer_rows.loc[layer_rows['prob_change'].idxmax()]\n",
    "        print(f\"Layer {layer_num:2d}: max change = {max_change_row['prob_change']:+.4f} \"\n",
    "              f\"({max_change_row['layer_name']})\")\n",
    "    \n",
    "    # Create a visualization showing layer effects\n",
    "    # We can use circuitsvis or plotly for this\n",
    "    print(\"\\nCreating layer effect visualization...\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
